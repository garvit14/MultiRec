{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cross_validation import train_test_split # to split the dataset for training and testing\n",
    "from sklearn.preprocessing import normalize # to normalize a matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from scipy import linalg\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import pyswarm\n",
    "from sklearn.model_selection import learning_curve # to analyze bias and variance of linear regression\n",
    "import matplotlib.pyplot as plt # for plotting the curves\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# getting the ratings data in pandas dataframe\n",
    "ratings = pd.read_csv('../Yahoo_movies_multi-criteria/data_movies.txt', sep='\\t',names=['user_id', 'criterion1', 'criterion2', 'criterion3', 'criterion4', 'overall', 'movie_id', 'num'])\n",
    "print(max(ratings.overall)) # implies that ratings are on a scale of 13\n",
    "# train, test = sample_split(ratings)\n",
    "# print(pd.Series.sort_values(ratings.movie_id).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users:  6078\n",
      "movies:  976\n"
     ]
    }
   ],
   "source": [
    "print(\"users: \",ratings.user_id.drop_duplicates().count()) #number of users\n",
    "print(\"movies: \",ratings.movie_id.drop_duplicates().count()) #number of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum movies rated by a user 5\n",
      "maximum movies rated by a user 237\n",
      "minimum ratings of a movie 1\n",
      "maximum ratings of a movie 830\n"
     ]
    }
   ],
   "source": [
    "print(\"minimum movies rated by a user\" ,min(ratings.groupby([\"user_id\"]).overall.count())) # minimum ratings given by any user\n",
    "print(\"maximum movies rated by a user\" ,max(ratings.groupby([\"user_id\"]).overall.count())) # maximum ratings given by any user\n",
    "print(\"minimum ratings of a movie\" ,min(ratings.groupby([\"movie_id\"]).overall.count())) # minimum ratings of a movie\n",
    "print(\"maximum ratings of a movie\" ,max(ratings.groupby([\"movie_id\"]).overall.count())) # maximum ratings of a movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to crop the dataset such that ratings given by any user>=ru\n",
    "def min_user_ratings_criteria(ratings, ru):\n",
    "    # remove all users who have rated <= ru movies\n",
    "    temp = ratings.groupby(['user_id']).overall.count().reset_index()\n",
    "#     print(temp)\n",
    "    users_to_be_removed = temp[temp.overall < ru].user_id.unique()\n",
    "    print(len(users_to_be_removed))\n",
    "    new_ratings = ratings[~ratings['user_id'].isin(users_to_be_removed)]\n",
    "    print(len(new_ratings.user_id.unique()))\n",
    "    return new_ratings\n",
    "#     print(len(users_to_be_removed))\n",
    "# min_ratings_criteria(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to crop the dataset such that every item is rated by atleast ri users\n",
    "def min_item_ratings_criteria(ratings, ri):\n",
    "    # remove all items with number of ratings <= ri\n",
    "    temp = ratings.groupby(['movie_id']).overall.count().reset_index()\n",
    "#     print(temp)\n",
    "    items_to_be_removed = temp[temp.overall < ri].movie_id.unique()\n",
    "    print(len(items_to_be_removed))\n",
    "    new_ratings = ratings[~ratings['movie_id'].isin(items_to_be_removed)]\n",
    "    print(len(new_ratings.movie_id.unique()))\n",
    "    return new_ratings\n",
    "# min_item_ratings_criteria(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the datasets into smaller datasets, such that every dataset contains 'size' number of users\n",
    "def split_dataset(ratings, size):\n",
    "        df_group = ratings.groupby(['user_id'])\n",
    "#         print(type(df_group))\n",
    "        \n",
    "        df_group_list = list()\n",
    "        # converting the group of dataframes into list of dataframe for random shuffling\n",
    "        for index, item in df_group:\n",
    "            df_group_list.append(item)\n",
    "        # randomly shuffling the list\n",
    "        random.shuffle(df_group_list)\n",
    "        no_of_datasets = int(len(df_group)/size);\n",
    "#         print(no_of_datasets)\n",
    "        list_datasets = list() # list containing all the small datasets\n",
    "        temp = pd.DataFrame()\n",
    "        count=0\n",
    "        flag=0\n",
    "        datasets_generated = 0 # datasets that have been generated till now\n",
    "        for item in df_group_list:\n",
    "            count+=1\n",
    "            temp = temp.append(item)\n",
    "            if count==size and datasets_generated < no_of_datasets-1:\n",
    "                list_datasets.append(temp)\n",
    "                print(\"size = \",temp.shape[0])\n",
    "#                 print(temp.user_id.unique())\n",
    "                temp = pd.DataFrame()\n",
    "                datasets_generated += 1\n",
    "                count=0\n",
    "        list_datasets.append(temp) # appending the last dataset\n",
    "        print(\"size = \",temp.shape[0])\n",
    "        return list_datasets\n",
    "    \n",
    "# testing above function\n",
    "# temp = split_dataset(ratings, 1000)\n",
    "# for item in temp:\n",
    "#     print(len(item.user_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spliting a sample into groups userwise\n",
    "def sample_split(dataFrame):\n",
    "    df_group = dataFrame.groupby('user_id')\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "    \n",
    "    for key, item in df_group:\n",
    "        train, test = split_train_test(item, train, test)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spliting sample in 70% training data and 30% testing data\n",
    "def split_train_test(dataFrame, train, test):\n",
    "    temp_train, temp_test = train_test_split(dataFrame, test_size = 0.3, random_state=1212)# in this our main data is split into train and test\n",
    "    # the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\n",
    "    train = train.append(temp_train)\n",
    "    test = test.append(temp_test)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size =  10663\n",
      "size =  9965\n",
      "size =  10190\n",
      "size =  9822\n",
      "size =  9934\n",
      "size =  11383\n",
      "(61957, 8)\n"
     ]
    }
   ],
   "source": [
    "# remove movies with <5 ratings\n",
    "# rating_dash = min_item_ratings_criteria(ratings, 5)\n",
    "# remove users who have rated <10 items\n",
    "# rating_dash = min_user_ratings_criteria(rating_dash, 10)\n",
    "# print(len(rating_dash.movie_id.unique()))\n",
    "# list conatining all the datasets\n",
    "list_datasets = split_dataset(rating_dash,1000)\n",
    "print(rating_dash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# splitting into train and test data\n",
    "train = list()\n",
    "test = list()\n",
    "num_of_datasets = len(list_datasets)\n",
    "print(num_of_datasets)\n",
    "for i in range(num_of_datasets):\n",
    "    print(i)\n",
    "    t1, t2 = sample_split(list_datasets[i])\n",
    "    train.append(t1)\n",
    "    test.append(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-criteria user-user collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to find similarity between two users based on Manhattan distance\n",
    "def manhattan_similarity(df, user1, user2):\n",
    "    s = pd.merge(df[df['user_id']==user1], df[df['user_id']==user2], how=\"inner\", on=\"movie_id\")\n",
    "#     print(s)\n",
    "    sum1 = 0\n",
    "    similarity=0\n",
    "    for index, row in s.iterrows():\n",
    "        sum1 += abs(row.criterion1_x - row.criterion1_y) + \\\n",
    "                abs(row.criterion2_x - row.criterion2_y) + \\\n",
    "                abs(row.criterion3_x - row.criterion3_y) + \\\n",
    "                abs(row.criterion4_x - row.criterion4_y) + \\\n",
    "                abs(row.overall_x - row.overall_y)\n",
    "        distance = sum1/s.shape[0]\n",
    "        similarity = 1/(1+distance)\n",
    "    return similarity\n",
    "        \n",
    "# manhattan_similarity(train, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to find simialrity between two users based on Euclidean distance\n",
    "def euclidean_similarity(df, user1, user2):\n",
    "    s = pd.merge(df[df['user_id']==user1], df[df['user_id']==user2], how=\"inner\", on=\"movie_id\")\n",
    "#     print(s)\n",
    "    sum1 = 0\n",
    "    similarity=0\n",
    "    for index, row in s.iterrows():\n",
    "        sum1 += math.sqrt((row.criterion1_x - row.criterion1_y)**2 + \\\n",
    "                (row.criterion2_x - row.criterion2_y)**2 + \\\n",
    "                (row.criterion3_x - row.criterion3_y)**2 + \\\n",
    "                (row.criterion4_x - row.criterion4_y)**2) + \\\n",
    "                (row.overall_x - row.overall_y)**2\n",
    "        distance = sum1/s.shape[0]\n",
    "        similarity = 1/(1+distance)\n",
    "    return similarity\n",
    "# euclidean_similarity(train, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to find simialrity between two users based on Chebyshev distance\n",
    "def chebyshev_similarity(df, user1, user2):\n",
    "    s = pd.merge(df[df['user_id']==user1], df[df['user_id']==user2], how=\"inner\", on=\"movie_id\")\n",
    "#     print(s)\n",
    "    sum1 = 0\n",
    "    similarity=0\n",
    "    for index, row in s.iterrows():\n",
    "        sum1 += max(abs(row.criterion1_x - row.criterion1_y), \\\n",
    "                abs(row.criterion2_x - row.criterion2_y), \\\n",
    "                abs(row.criterion3_x - row.criterion3_y), \\\n",
    "                abs(row.criterion4_x - row.criterion4_y), \\\n",
    "                abs(row.overall_x - row.overall_y))\n",
    "        distance = sum1/s.shape[0]\n",
    "        similarity = 1/(1+distance)\n",
    "    return similarity\n",
    "        \n",
    "# chebyshev_similarity(train, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to predict the rating given by user to item\n",
    "# neighbours = -1 implies that consider similarity with all possible users\n",
    "def predict(df, user, item, similarity, neighbours = -1): # df is the train dataset\n",
    "    neighbours_data_list = list()\n",
    "    for v in df.user_id.unique():\n",
    "        if(v==user): # not including the user itself\n",
    "            continue\n",
    "        temp = df[df['user_id']==v]\n",
    "        temp = temp[temp['movie_id']==item]\n",
    "        if(temp.empty): # user 'v' has not rated the item\n",
    "            continue\n",
    "        else:\n",
    "            rate = temp.iloc[0].overall\n",
    "        sim = similarity(df, user, v) # find appropriate similarity measure between the two users\n",
    "        neighbours_data_list.append((sim, rate))\n",
    "        \n",
    "    # sort the neighbours_data_list in descending order based on rate\n",
    "    neighbours_data_list.sort(reverse=True)\n",
    "    \n",
    "    # crop the list to the number of neighbours given in the argument\n",
    "    if(neighbours!=-1):\n",
    "        length = len(neighbours_data_list)\n",
    "        neighbours_data_list = neighbours_data_list[:min(neighbours, length)]\n",
    "    \n",
    "    # predict the rating using collaborative filtering formula\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    if(len(neighbours_data_list)==0):\n",
    "        print(\"We cannot predict\")\n",
    "    for tup in neighbours_data_list:\n",
    "        numerator += tup[0]*tup[1]\n",
    "        denominator += tup[0]\n",
    "    if(denominator==0):\n",
    "        return 0\n",
    "    predicted_rating = numerator/denominator\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(test[0].head(26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# print(train)\n",
    "print(predict(train[0], 1, 879, euclidean_similarity))\n",
    "print(predict(train[0], 1, 879, manhattan_similarity))\n",
    "print(predict(train[0], 1, 879, chebyshev_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to calculate the mean average error between two vectors\n",
    "def mean_average_error(list1, list2):\n",
    "    sum1=0\n",
    "    for a,b in zip(list1,list2):\n",
    "        if(b==0):            # to handle the unpredicted ratings\n",
    "            continue\n",
    "        sum1 += abs(a-b)\n",
    "    error = sum1/len(list1)\n",
    "    return error\n",
    "mean_average_error([1,2,3],[10,20,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to replace entries in a list with zero/one based on some threshold\n",
    "def replace_with_zero_one(list1, threshold):\n",
    "    length = len(list1)\n",
    "    rlist = list()\n",
    "    i = 0\n",
    "    while i < length:\n",
    "        if(list1[i]==0):   # to handle the unpredicted ratings\n",
    "            list1[i]=-1\n",
    "            continue\n",
    "        if(list1[i]>threshold):\n",
    "            rlist.append(1)\n",
    "        else:\n",
    "            rlist.append(0)\n",
    "        i += 1\n",
    "    return rlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate the precision between two lists\n",
    "# list2 is the predicted list\n",
    "def precision(list1, list2, threshold): # values > threshold are considered as good values\n",
    "#     list1 = replace_with_zero_one(list1, threshold)\n",
    "#     list2 = replace_with_zero_one(list2, threshold)\n",
    "    tp = 0 # true positives\n",
    "    fp = 0 # false positives\n",
    "    for a,b in zip(list1, list2):\n",
    "        if(a==1 and b==1):\n",
    "            tp += 1\n",
    "        if(b==1 and a==0):\n",
    "            fp+=1\n",
    "    precision = tp/(tp + fp)\n",
    "    return precision\n",
    "\n",
    "# precision([20,5,14], [1,16,20], 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate the recall between two lists\n",
    "# list2 is the predicted list\n",
    "def recall(list1, list2, threshold): # values > threshold are considered as good values\n",
    "#     list1 = replace_with_zero_one(list1, threshold)\n",
    "#     list2 = replace_with_zero_one(list2, threshold)\n",
    "    tp = 0 # true positives\n",
    "    fn = 0 # false negatives\n",
    "    for a,b in zip(list1, list2):\n",
    "        if(a==1 and b==1):\n",
    "            tp += 1\n",
    "        if(b==0 and a==1):\n",
    "            fn += 1\n",
    "#     print(\"tp = \",tp)\n",
    "#     print(\"fn = \",fn)\n",
    "    recall = tp/(tp + fn)\n",
    "    return recall\n",
    "\n",
    "# recall([20,15,14], [1,16,15], 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate F1-score between two lists\n",
    "def f1_score(list1, list2, threshold):\n",
    "    p = precision(list1, list2, threshold)\n",
    "    r = recall(list1, list2, threshold)\n",
    "    f1 = (2*p*r)/(p+r)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicting on our training dataset\n",
    "predicted = list() # for storing the predicted ratings\n",
    "# appending 7 empty lists\n",
    "i=0\n",
    "while i<len(list_datasets):\n",
    "    predicted.append([])\n",
    "    i+=1\n",
    "unpredicted_count = list()\n",
    "# print(predicted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "0\n",
      "9.34528605963\n",
      "4.04347826087\n",
      "11.25\n",
      "We cannot predict\n",
      "0\n",
      "0\n",
      "0\n",
      "13.0\n",
      "9.0\n",
      "9.84158415842\n",
      "10.0794775293\n",
      "12.1232032854\n",
      "11.9938650307\n",
      "10.3560797529\n",
      "12.2122356495\n",
      "0\n",
      "0\n",
      "5.0\n",
      "10.9189189189\n",
      "10.5\n",
      "10.2988854286\n",
      "We cannot predict\n",
      "0\n",
      "10.0769230769\n",
      "4.83495145631\n",
      "11.2195121951\n",
      "0\n",
      "8.5\n",
      "11.0324324324\n",
      "7.90909090909\n",
      "10.1956521739\n",
      "1.0\n",
      "9.10160275661\n",
      "0\n",
      "8.95625635809\n",
      "11.4613402062\n",
      "9.82075471698\n",
      "11.8722891566\n",
      "12.5613577023\n",
      "11.0\n",
      "10.0\n",
      "11.4285714286\n",
      "6.89156626506\n",
      "8.95652173913\n",
      "7.95652173913\n",
      "9.42857142857\n",
      "11.6071428571\n",
      "11.2\n",
      "5.1186440678\n",
      "We cannot predict\n",
      "0\n",
      "11.132078505\n",
      "9.203125\n",
      "7.0487804878\n",
      "9.05515451375\n",
      "7.42857142857\n",
      "7.0\n",
      "11.887822226\n",
      "6.76923076923\n",
      "4.85714285714\n",
      "11.0801409071\n",
      "11.9465747519\n",
      "8.1948376353\n",
      "10.5135135135\n",
      "10.0\n",
      "0\n",
      "5.36666666667\n",
      "9.0\n",
      "6.2\n",
      "9.66666666667\n",
      "10.4\n",
      "8.2\n",
      "7.0\n",
      "13.0\n",
      "4.875\n",
      "0\n",
      "12.0\n",
      "12.1208791209\n",
      "11.5591127155\n",
      "0\n",
      "4.0\n",
      "7.74747474747\n",
      "6.0\n",
      "0\n",
      "12.4412269004\n",
      "11.1258709309\n",
      "10.2\n",
      "11.0\n",
      "9.58536585366\n",
      "11.6379444589\n",
      "5.79560602623\n",
      "8.175066313\n",
      "10.4482315113\n",
      "10.063469109\n",
      "8.35709220283\n",
      "9.52525914504\n",
      "11.8188861219\n",
      "10.3054791546\n",
      "9.24340062112\n",
      "We cannot predict\n",
      "0\n",
      "11.1634423765\n",
      "9.25463834364\n",
      "10.4784593577\n",
      "10.495049505\n",
      "7.4060324826\n",
      "11.4170541276\n",
      "12.3809389935\n",
      "12.0\n",
      "We cannot predict\n",
      "0\n",
      "2.7\n",
      "11.9477055469\n",
      "11.3194328607\n",
      "0\n",
      "9.55029585799\n",
      "9.63795423957\n",
      "9.09302325581\n",
      "7.03666799522\n",
      "11.7149016881\n",
      "7.0\n",
      "12.1460356584\n",
      "10.8855455049\n",
      "12.5384615385\n",
      "7.75\n",
      "11.3119266055\n",
      "10.84760596\n",
      "10.2745471878\n",
      "10.7046447896\n",
      "13.0\n",
      "12.0\n",
      "3.66666666667\n",
      "8.92307692308\n",
      "6.61493882807\n",
      "11.8914571317\n",
      "12.3601122189\n",
      "13.0\n",
      "2.0\n",
      "0\n",
      "9.17944422546\n",
      "9.76647206005\n",
      "8.17142857143\n",
      "10.0728476821\n",
      "0\n",
      "11.1143073429\n",
      "12.4391457297\n",
      "9.69784172662\n",
      "5.21226415094\n",
      "3.65648854962\n",
      "8.68421052632\n",
      "5.77777777778\n",
      "7.8894382852\n",
      "1.0\n",
      "1.61111111111\n",
      "We cannot predict\n",
      "0\n",
      "9.0\n",
      "11.5682452667\n",
      "9.33333333333\n",
      "10.13949849\n",
      "11.0103092784\n",
      "9.69381242345\n",
      "10.0\n",
      "11.0\n",
      "0\n",
      "9.50980392157\n",
      "11.9682875264\n",
      "11.5734536082\n",
      "12.2222222222\n",
      "11.0\n",
      "11.2380952381\n",
      "8.66666666667\n",
      "10.0\n",
      "8.87484423893\n",
      "8.0\n",
      "12.2230390802\n",
      "12.1407407407\n",
      "9.85714285714\n",
      "0\n",
      "0\n",
      "9.05263157895\n",
      "9.0\n",
      "9.0\n",
      "9.90909090909\n",
      "6.0\n",
      "0\n",
      "6.64705882353\n",
      "10.0\n",
      "9.0\n",
      "8.71428571429\n",
      "11.6666666667\n",
      "11.4\n",
      "9.0\n",
      "8.14035087719\n",
      "5.66666666667\n",
      "5.0\n",
      "11.1764705882\n",
      "8.52929397326\n",
      "0\n",
      "9.59627329193\n",
      "7.0\n",
      "8.77314945117\n",
      "12.0334453526\n",
      "4.63414634146\n",
      "0\n",
      "10.5\n",
      "10.3043478261\n",
      "12.4666666667\n",
      "13.0\n",
      "8.79310344828\n",
      "12.0120385233\n",
      "9.27299263163\n",
      "12.6666666667\n",
      "11.0714285714\n",
      "0\n",
      "11.0489838458\n",
      "11.3703703704\n",
      "7.65120161747\n",
      "9.82309434968\n",
      "8.27586206897\n",
      "10.3853211009\n",
      "12.0\n",
      "0\n",
      "10.75\n",
      "5.31914893617\n",
      "0\n",
      "7.26132864056\n",
      "2.41176470588\n",
      "10.0\n",
      "11.650440889\n",
      "0\n",
      "12.3796296296\n",
      "11.9310344828\n",
      "12.0\n",
      "10.523902439\n",
      "10.3076551635\n",
      "9.88208677136\n",
      "0\n",
      "11.2920353982\n",
      "10.8450184502\n",
      "11.2456140351\n",
      "8.0\n",
      "0\n",
      "1.0\n",
      "12.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "11.231884058\n",
      "12.0\n",
      "10.2686567164\n",
      "10.3666666667\n",
      "11.7411444142\n",
      "0\n",
      "7.84198113208\n",
      "2.44\n",
      "1.0\n",
      "5.81720430108\n",
      "10.9429849365\n",
      "9.91279069767\n",
      "12.5097704015\n",
      "11.6284660971\n",
      "11.6325096147\n",
      "11.6438848921\n",
      "12.0957435858\n",
      "8.26460990804\n",
      "11.4865352856\n",
      "10.4571428571\n",
      "8.59764420178\n",
      "11.1639571286\n",
      "9.86484213343\n",
      "We cannot predict\n",
      "0\n",
      "11.2533039648\n",
      "11.5315664603\n",
      "6.03720930233\n",
      "0\n",
      "12.0\n",
      "6.72727272727\n",
      "7.71428571429\n",
      "0\n",
      "12.0\n",
      "10.7520661157\n",
      "0\n",
      "12.3014019369\n",
      "11.9649879894\n",
      "12.0909090909\n",
      "12.2608695652\n",
      "7.59405940594\n",
      "9.81553398058\n",
      "11.6875\n",
      "10.8241155954\n",
      "12.1447963801\n",
      "9.0\n",
      "12.0\n",
      "10.9501779359\n",
      "11.0\n",
      "10.2\n",
      "0\n",
      "11.8163265306\n",
      "11.9321428571\n",
      "11.9627791563\n",
      "11.8106152563\n",
      "5.5\n",
      "9.28571428571\n",
      "12.0892018779\n",
      "10.649122807\n",
      "9.0\n",
      "6.85714285714\n",
      "10.3524581655\n",
      "7.56928838951\n",
      "12.3855421687\n",
      "12.0\n",
      "0\n",
      "4.14285714286\n",
      "7.7435080022\n",
      "8.42785052326\n",
      "10.0839353084\n",
      "0\n",
      "10.4596484981\n",
      "10.4706046751\n",
      "8.70930232558\n",
      "10.3931363969\n",
      "8.82520432375\n",
      "10.4\n",
      "9.44576334951\n",
      "8.71328671329\n",
      "0\n",
      "13.0\n",
      "10.5954671107\n",
      "9.86971235195\n",
      "11.7554858934\n",
      "12.0243902439\n",
      "9.5\n",
      "We cannot predict\n",
      "0\n",
      "6.03896103896\n",
      "0\n",
      "9.31623931624\n",
      "0\n",
      "10.3676926772\n",
      "1.0\n",
      "7.0\n",
      "11.0\n",
      "10.604918771\n",
      "We cannot predict\n",
      "0\n",
      "11.0345342542\n",
      "12.4071146245\n",
      "8.63106796117\n",
      "9.68681863231\n",
      "11.8624838421\n",
      "0\n",
      "12.0\n",
      "10.7949615366\n",
      "10.5899773892\n",
      "8.61673838739\n",
      "5.64041994751\n",
      "8.64487655111\n",
      "11.4114910414\n",
      "7.81810187216\n",
      "10.2414228385\n",
      "7.68683274021\n",
      "11.5973575915\n",
      "9.9537006538\n",
      "9.0\n",
      "10.0\n",
      "8.55848580442\n",
      "10.1557811611\n",
      "8.45468045623\n",
      "10.8370781406\n",
      "10.3995243757\n",
      "10.8099173554\n",
      "11.0267337535\n",
      "12.0874872838\n",
      "11.1347895578\n",
      "12.3333333333\n",
      "11.8982511924\n",
      "11.8637911464\n",
      "8.67741935484\n",
      "11.7435721498\n",
      "12.7563805104\n",
      "12.3621787487\n",
      "12.0\n",
      "13.0\n",
      "12.1735979253\n",
      "11.4442800789\n",
      "10.3208187544\n",
      "0\n",
      "7.0\n",
      "0\n",
      "9.28571428571\n",
      "7.38845604168\n",
      "10.855227882\n",
      "7.3\n",
      "12.1016042781\n",
      "7.42335766423\n",
      "11.2349697034\n",
      "11.4392716351\n",
      "8.86486486486\n",
      "8.2840500257\n",
      "10.7142857143\n",
      "7.77303736356\n",
      "10.915981917\n",
      "11.1943284708\n",
      "3.29725317117\n",
      "10.5040871935\n",
      "7.25909752547\n",
      "10.2913172208\n",
      "9.23956786207\n",
      "4.67138072199\n",
      "8.18655097614\n",
      "9.33333333333\n",
      "4.6\n",
      "3.59459459459\n",
      "8.44560075686\n",
      "6.194000609\n",
      "10.576777643\n",
      "11.6796875\n",
      "0\n",
      "11.3203181216\n",
      "10.0\n",
      "12.2689655172\n",
      "0\n",
      "9.57142857143\n",
      "11.6666666667\n",
      "0\n",
      "We cannot predict\n",
      "0\n",
      "8.0989010989\n",
      "0\n",
      "10.2211055276\n",
      "10.8235294118\n",
      "11.8387096774\n",
      "9.25653206651\n",
      "11.984919452\n",
      "10.0\n",
      "10.7841651424\n",
      "10.8328021164\n",
      "6.99673735726\n",
      "5.94117647059\n",
      "11.6510068562\n",
      "9.59673258813\n",
      "9.77247353224\n",
      "8.76348435814\n",
      "6.49962738642\n",
      "10.6725285171\n",
      "10.6063119243\n",
      "7.40537913156\n",
      "10.5737704918\n",
      "5.66666666667\n",
      "11.1498127341\n",
      "11.5325884544\n",
      "11.6551724138\n",
      "11.9333333333\n",
      "12.0\n",
      "0\n",
      "11.6209677419\n",
      "13.0\n",
      "0\n",
      "8.22454308094\n",
      "12.0\n",
      "9.89855072464\n",
      "6.23529411765\n",
      "12.270400961\n",
      "9.98944491169\n",
      "8.87726612811\n",
      "12.0784464697\n",
      "6.89285714286\n",
      "11.4560260586\n",
      "10.9322033898\n",
      "13.0\n",
      "0\n",
      "8.60944206009\n",
      "5.47611827142\n",
      "8.90462427746\n",
      "12.4054054054\n",
      "8.51612903226\n",
      "0\n",
      "9.77142857143\n",
      "10.6977777778\n",
      "11.4640957447\n",
      "0\n",
      "11.1953820081\n",
      "11.035672369\n",
      "8.53746978243\n",
      "4.34094151213\n",
      "9.4\n",
      "10.8275862069\n",
      "12.1294117647\n",
      "6.27272727273\n",
      "7.44736842105\n",
      "0\n",
      "We cannot predict\n",
      "0\n",
      "11.1641253544\n",
      "9.77962250961\n",
      "12.0\n",
      "11.0851474719\n",
      "10.7305510535\n",
      "11.2253521127\n",
      "0\n",
      "9.29399585921\n",
      "10.9760137253\n",
      "10.6311055978\n",
      "11.1219209273\n",
      "8.93753684459\n",
      "10.9878416051\n",
      "8.0214941931\n",
      "3.5\n",
      "12.6946564885\n",
      "1.0\n",
      "9.0\n",
      "10.7261906385\n",
      "4.47000582411\n",
      "12.0\n",
      "We cannot predict\n",
      "0\n",
      "8.17650962177\n",
      "0\n",
      "0\n",
      "11.0\n",
      "11.9231908779\n",
      "10.0905432596\n",
      "12.2807424594\n",
      "12.3684210526\n",
      "12.9285714286\n",
      "11.0682953914\n",
      "10.8952654232\n",
      "12.0636942675\n",
      "13.0\n",
      "10.8785519657\n",
      "7.38255114806\n",
      "0\n",
      "11.2365415987\n",
      "11.0420677921\n",
      "7.31507876969\n",
      "10.7929334429\n",
      "12.4827155587\n",
      "11.3126612249\n",
      "10.2929292929\n",
      "8.75438596491\n",
      "4.00278551532\n",
      "11.2751277683\n",
      "9.90476190476\n",
      "4.12482468443\n",
      "10.0\n",
      "9.44545454545\n",
      "9.0\n",
      "1.88636363636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6b05ac2d83fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mpredicted_rating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmovie_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchebyshev_similarity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#         print(predicted_rating)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_rating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-403e8bf3edfc>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(df, user, item, similarity, neighbours)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# not including the user itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'movie_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# user 'v' has not rated the item\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other, axis)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m                 raise TypeError('Could not compare %s type with Series' %\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    796\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"invalid type comparison\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# bulk testing on all datasets\n",
    "for i in range(len(list_datasets)):\n",
    "    cnt = 0\n",
    "    for index, row in test[i].iterrows():\n",
    "        predicted_rating = predict(train[i], row.user_id, row.movie_id,chebyshev_similarity)\n",
    "#         print(predicted_rating)\n",
    "        if(math.isnan(predicted_rating)):\n",
    "            print(\"Cannot predict\")\n",
    "            continue\n",
    "        if(predicted_rating==0):\n",
    "            cnt += 1\n",
    "        predicted[i].append(predicted_rating)\n",
    "        print(predicted_rating)\n",
    "    print(\"predicted %d\",i)\n",
    "    unpredicted_count.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-size =  3646\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Continuing\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b53186e48846>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpredicted_rating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmovie_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meuclidean_similarity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-403e8bf3edfc>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(df, user, item, similarity, neighbours)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'movie_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# user 'v' has not rated the item\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1998\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1999\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2000\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2001\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2002\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, convert, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   1926\u001b[0m         new_data = self._data.take(indices,\n\u001b[0;32m   1927\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1928\u001b[1;33m                                    convert=True, verify=True)\n\u001b[0m\u001b[0;32m   1929\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   4007\u001b[0m                                 'the axis length')\n\u001b[0;32m   4008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4009\u001b[1;33m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4010\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[0;32m   4011\u001b[0m                                     axis=axis, allow_dups=True)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[0;32m   1785\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Unable to fill values because {0} cannot contain NA'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m             \u001b[0mtaken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1788\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shallow_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# predicting on our training dataset1\n",
    "print(\"test-size = \", test[1].shape[0])\n",
    "predicted[1] = list() # for storing the predicted ratings\n",
    "i=0\n",
    "for index, row in test[1].iterrows():\n",
    "    predicted_rating = predict(train[1], row.user_id, row.movie_id,euclidean_similarity, 20)\n",
    "    print(i)\n",
    "    i+=1\n",
    "    if(math.isnan(predicted_rating) or predicted_rating==0):\n",
    "        print(\"Continuing\")\n",
    "        continue\n",
    "    predicted[1].append(predicted_rating)\n",
    "print(predicted[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "mae =  0.00686817891691\n",
      "precision =  0.8571428571428571\n",
      "recall =  0.75\n",
      "f1-score =  0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "# print(\"unpredicted = \",unpredicted_count[1])\n",
    "print(len(predicted[1]))\n",
    "threshold = 7\n",
    "ttest = test[1].overall\n",
    "list1 = replace_with_zero_one(ttest.tolist(), threshold)\n",
    "list2 = replace_with_zero_one(predicted[1], threshold)\n",
    "# mean average error\n",
    "print(\"mae = \",mean_average_error(ttest, predicted[1]))\n",
    "# precision\n",
    "print(\"precision = \",precision(list1, list2, threshold))\n",
    "# recall\n",
    "print(\"recall = \",recall(list1, list2, threshold))\n",
    "# f1 score\n",
    "print(\"f1-score = \",f1_score(list1, list2, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to convert data into mxn matrix m -> number of users, n -> number of movies\n",
    "# the matrix is normalized to take care of missing ratings\n",
    "# after normalizations missing ratings can be safely assumed to be zero\n",
    "# df dataframe should contain columns as 'user_id', 'movie_id', 'rate'\n",
    "def convert_to_normalized_matrix(df, rating_column):\n",
    "    no_of_movies = len(ratings.movie_id.unique())\n",
    "    no_of_users = len(ratings.user_id.unique())\n",
    "    matrix = np.zeros((no_of_users, no_of_movies))\n",
    "    for index, row in df.iterrows():\n",
    "        matrix[row.user_id-1][row.movie_id-1] = row[rating_column]\n",
    "#     print(\"completed\")\n",
    "    return matrix,normalize_matrix(matrix)\n",
    "\n",
    "# function to normalize a matrix by subtracting the average rating of the user\n",
    "def normalize_matrix(matrix):\n",
    "#     matrix[matrix == 0] = np.nan\n",
    "#     matrix = np.ma.array(matrix, mask=np.isnan(matrix))\n",
    "\n",
    "    # normalize the matrix\n",
    "    normalized_matrix = np.zeros((len(matrix), len(matrix[0])))\n",
    "    i=0\n",
    "#     print(matrix)\n",
    "    num_users = len(matrix)\n",
    "    while i < num_users:\n",
    "        baseline = np.sum(matrix[i])\n",
    "        baseline /= np.count_nonzero(matrix[i])\n",
    "        # subtract baseline value from all non-zero ratings\n",
    "        normalized_matrix[i][:] = [x-baseline if x>0 else x for x in matrix[i]]\n",
    "        i+=1\n",
    "    return normalized_matrix\n",
    "    \n",
    "    \n",
    "# g,h = convert_to_normalized_matrix(ratings[['user_id', 'movie_id', 'overall']])\n",
    "# print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to apply SVD on normalized user-item matrix\n",
    "def svd(ratings, rating_column):\n",
    "    original,X = convert_to_normalized_matrix(ratings[['user_id', 'movie_id', rating_column]], rating_column)\n",
    "    print(\"matrix created, now decomposing it\")\n",
    "    u,s,v = np.linalg.svd(X) # Singular Value Decomposition\n",
    "    return original,u,s,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to crop the decomposed matrices to top-k latent features\n",
    "def crop_matrices(u, s, v, k):\n",
    "    # calculate average rating of the user according to 'matrix'\n",
    "    cropped_u = u[:,:k] # crop matrix u\n",
    "    cropped_s = s[:k] # crop array s\n",
    "    cropped_v = v[:k,:] # crop matrix v\n",
    "    return cropped_u, cropped_s, cropped_v\n",
    "# crop_matrices(u,s,v,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to predict the rating given by 'user' to 'item' based on SVD\n",
    "# u, s, v are the decomposed matrices\n",
    "# matrix is the parent matrix that is decomposed\n",
    "def predict_svd(matrix, u, s, v, user, item):\n",
    "    # calculate average rating of 'user' according to 'matrix'\n",
    "    baseline = np.sum(matrix[user-1])\n",
    "    baseline /= np.count_nonzero(matrix[user-1])\n",
    "#     print(\"average -> \", baseline)\n",
    "    p = u[user-1] # latent-features values of the user\n",
    "    # s is the weight of latent-features\n",
    "    q = np.transpose(v)[item-1]\n",
    "    \n",
    "    return baseline+sum(p*s*q)\n",
    "    \n",
    "# matrix,u,s,v = svd(ratings,'overall')\n",
    "# print(\"completed\")\n",
    "# a,b,c = crop_matrices(u,s,v,15)\n",
    "# print(\"completed\")\n",
    "# predict_svd(matrix,a,b,c,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garvit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix created, now decomposing it\n"
     ]
    }
   ],
   "source": [
    "# creating the decomposed matrices\n",
    "matrix,u,s,v = svd(train[0], 'overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# croppping the matrices to appropriate no. of latent-features\n",
    "a,b,c = crop_matrices(u,s,v,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-size =  3433\n"
     ]
    }
   ],
   "source": [
    "# testing the svd implementation\n",
    "# fingers crossed\n",
    "# predicting on our training dataset1\n",
    "print(\"test-size = \", test[0].shape[0])\n",
    "predicted[0] = list() # for storing the predicted ratings\n",
    "i=0\n",
    "for index, row in test[0].iterrows():\n",
    "    predicted_rating = predict_svd(matrix,a,b,c,row.user_id, row.movie_id)\n",
    "#     print(i)\n",
    "    i+=1\n",
    "    if(math.isnan(predicted_rating) or predicted_rating==0):\n",
    "        print(\"Continuing\")\n",
    "        continue\n",
    "    predicted[0].append(predicted_rating)\n",
    "# print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating empty lists for storing the predicted values\n",
    "predicted = list() # for storing the predicted ratings\n",
    "i=0\n",
    "while i<len(list_datasets):\n",
    "    predicted.append([])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garvit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix created, now decomposing it\n",
      "predicted  0\n",
      "matrix created, now decomposing it\n",
      "predicted  1\n",
      "matrix created, now decomposing it\n",
      "predicted  2\n",
      "matrix created, now decomposing it\n",
      "predicted  3\n",
      "matrix created, now decomposing it\n",
      "predicted  4\n",
      "matrix created, now decomposing it\n",
      "predicted  5\n"
     ]
    }
   ],
   "source": [
    "# bulk testing on all datasets\n",
    "for i in range(len(list_datasets)):\n",
    "    matrix,u,s,v = svd(train[i], 'overall')\n",
    "    a,b,c = crop_matrices(u,s,v,15)\n",
    "    for index, row in test[i].iterrows():\n",
    "        predicted_rating = predict_svd(matrix,a,b,c,row.user_id, row.movie_id)\n",
    "        if(math.isnan(predicted_rating)):\n",
    "            print(\"Cannot predict\")\n",
    "            continue\n",
    "        if(predicted_rating==0):\n",
    "            cnt += 1\n",
    "        predicted[i].append(predicted_rating)\n",
    "#         print(predicted_rating)\n",
    "    print(\"predicted \",i)\n",
    "#     unpredicted_count.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696\n",
      "mae =  2.64854391853\n",
      "precision =  0.7879243015920697\n",
      "recall =  0.9258736321920226\n",
      "f1-score =  0.8513469652710159\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "print(len(predicted[5]))\n",
    "threshold = 7\n",
    "ttest = test[5].overall\n",
    "list1 = replace_with_zero_one(ttest.tolist(), threshold)\n",
    "list2 = replace_with_zero_one(predicted[5], threshold)\n",
    "# mean average error\n",
    "print(\"mae = \",mean_average_error(ttest, predicted[5]))\n",
    "# precision\n",
    "print(\"precision = \",precision(list1, list2, threshold))\n",
    "# recall\n",
    "print(\"recall = \",recall(list1, list2, threshold))\n",
    "# f1 score\n",
    "print(\"f1-score = \",f1_score(list1, list2, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimization (PSO)\n",
    "## (using the pyswarm library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the fitness funcion to be optimised by pso\n",
    "# weights is the list of weights that are to be calculated by PSO\n",
    "# 'user' is the user for which we want to predict the weights\n",
    "user = pd.DataFrame()\n",
    "def fitness_func(weights):\n",
    "    w1 = weights[0]\n",
    "    w2 = weights[1]\n",
    "    w3 = weights[2]\n",
    "    w4 = weights[3]\n",
    "    ans = 0\n",
    "    for index, row in user.iterrows():\n",
    "        ans += (w1*row.criterion1 + w2*row.criterion2 + w3*row.criterion3 + w4*row.criterion4 - row.overall)**2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for predicting the label according to our weights got using pso\n",
    "def predict_pso(xopt, test):\n",
    "    predicted_list = list()\n",
    "    w1 = xopt[0]\n",
    "    w2 = xopt[1]\n",
    "    w3 = xopt[2]\n",
    "    w4 = xopt[3]\n",
    "    for index, row in test.iterrows():\n",
    "        prediction = w1*row.criterion1 + w2*row.criterion2 + w3*row.criterion3 + w4*row.criterion4\n",
    "        predicted_list.append(prediction)\n",
    "    return predicted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anubhav's implementation of PSO\n",
    "def pso(fitness_func, lbound, ubound, swarm_size=20, max_iter=40, dimensions=4, phip=0.5, phig=0.5, weight=0.017):\n",
    "    x=[]            # positon of each particle\n",
    "    x_pbest=[]      # best position of each particle \n",
    "    x_gbest=[]      # best global position of population\n",
    "    v=[]            # velocity of each particle\n",
    "    v_min=[]\n",
    "    v_max=[]\n",
    "    \n",
    "    # assign initial random positions to the particles\n",
    "    for i in range(swarm_size):\n",
    "        x.append([])\n",
    "        x_pbest.append([])\n",
    "        for j in range(dimensions):\n",
    "            x[i].append(random.uniform(lbound[j],ubound[j]))\n",
    "            x_pbest[i].append(x[i][j])\n",
    "            \n",
    "    #calculate initial group best of the population\n",
    "    for i in range(swarm_size):\n",
    "        if i==0:\n",
    "            x_gbest=x[i]\n",
    "        elif fitness_func(x[i])<fitness_func(x_gbest):\n",
    "            x_gbest=x[i]\n",
    "            \n",
    "            \n",
    "    r1=random.uniform(0,1)\n",
    "    r2=random.uniform(0,1)\n",
    "    \n",
    "    # calculate minimum and maximum boundaries of velocity vector\n",
    "    for i in range(dimensions):\n",
    "        v_min.append(-(ubound[i]-lbound[i])/10)\n",
    "        v_max.append((ubound[i]-lbound[i])/10)\n",
    " \n",
    "    # assign initial random velocities to the particles\n",
    "    for i in range(swarm_size):\n",
    "        v.append([])\n",
    "        for j in range(dimensions):\n",
    "            v[i].append(random.uniform(v_min[j],v_max[j]))\n",
    "            \n",
    "    for iter in range(max_iter):\n",
    "        for i in range(swarm_size):\n",
    "            for j in range(dimensions):\n",
    "                # calculate new velocity for each particle\n",
    "                v[i][j] = weight*(v[i][j]) + r1*phip*(x_pbest[i][j]-x[i][j]) + r2*phig*(x_gbest[j]-x[i][j])\n",
    "                \n",
    "#                 if v[i][j] > v_max[j]:\n",
    "#                     v[i][j] = v_max[j]\n",
    "                \n",
    "#                 if v[i][j] < v_min[j]:\n",
    "#                     v[i][j] = v_min[j]\n",
    "\n",
    "\n",
    "                # Kumud's code\n",
    "                #checking the limit of velocity, if goes beyond then sack it\n",
    "                if v[i][j] > v_max:\n",
    "                    vel[i][j] = (vel_max / abs(vel[i][j])*vel[i][j])\n",
    "                                 \n",
    "                                 \n",
    "                \n",
    "                # calculate new position for each particle\n",
    "                x[i][j] = x[i][j] + v[i][j]\n",
    "                \n",
    "                if x[i][j] > ubound[j]:\n",
    "                    x[i][j] = ubound[j]\n",
    "                    \n",
    "                if x[i][j] < lbound[j]:\n",
    "                    x[i][j] = lbound[j]\n",
    "            \n",
    "            if fitness_func(x[i]) < fitness_func(x_pbest[i]):\n",
    "                x_pbest[i] = x[i]\n",
    "                \n",
    "            if fitness_func(x[i]) < fitness_func(x_gbest):\n",
    "                x_gbest = x[i]\n",
    "                \n",
    "    return x_gbest,fitness_func(x_gbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0]\n",
      "264\n",
      "mae =  5.5\n",
      "[0, 0, 0, 0]\n",
      "306\n",
      "mae =  12.5\n",
      "[0, 63.90047939062116, 0, 16.663029575979127]\n",
      "2749424.80397\n",
      "mae =  901.719768288\n",
      "[0, 0, 42.18954725523602, 0.6950620000336839]\n",
      "459901.78912\n",
      "mae =  148.181318394\n",
      "[0, 0, 0, 0]\n",
      "402\n",
      "mae =  13.0\n",
      "[0, 0, 0, 0]\n",
      "171\n",
      "mae =  12.5\n",
      "[0, 0, 19.409921877282674, 0]\n",
      "353437.001777\n",
      "mae =  146.612708352\n",
      "[0, 0, 0, 0]\n",
      "492\n",
      "mae =  12.0\n",
      "[0, 0, 0, 0]\n",
      "393\n",
      "mae =  10.0\n",
      "[0, 0, 0, 0]\n",
      "1161\n",
      "mae =  9.8\n",
      "[0, 0, 0, 0]\n",
      "603\n",
      "mae =  8.5\n",
      "[0, 0, 0, 0]\n",
      "492\n",
      "mae =  6.5\n",
      "[0, 0, 0, 0]\n",
      "351\n",
      "mae =  5.66666666667\n",
      "[0, 0, 0, 0]\n",
      "371\n",
      "mae =  5.5\n",
      "[0, 0, 0, 0]\n",
      "486\n",
      "mae =  11.0\n",
      "[0, 0, 0, 0]\n",
      "460\n",
      "mae =  11.0\n",
      "[0, 0, 0, 0]\n",
      "557\n",
      "mae =  12.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-0ecf6466fcd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#     xopt, fopt = pyswarm.pso(fitness_func, lower_bound, upper_bound)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Anubhav's implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mxopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitness_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-163-9bd625a468c2>\u001b[0m in \u001b[0;36mpso\u001b[1;34m(fitness_func, lbound, ubound, swarm_size, max_iter, dimensions, phip, phig, weight)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mx_pbest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mfitness_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mfitness_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_gbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m                 \u001b[0mx_gbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-8e1f22233e68>\u001b[0m in \u001b[0;36mfitness_func\u001b[1;34m(weights)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mans\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion4\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3078\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   2472\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2474\u001b[1;33m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'getitem'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2475\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2476\u001b[0m             return self._engine.get_value(s, k,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[1;34m(self, key, kind)\u001b[0m\n\u001b[0;32m   1301\u001b[0m     \"\"\"\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1303\u001b[1;33m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_index_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_convert_scalar_indexer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ix'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'getitem'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'iloc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# applying the particle-swarm-optimization\n",
    "# xopt = the optimal input values\n",
    "# fopt = the optimal objective value\n",
    "df = list_datasets[0]\n",
    "user_list = df.groupby(['user_id'])\n",
    "lower_bound = [0, 0, 0, 0]\n",
    "upper_bound = [255, 255, 255, 255]\n",
    "mae = 0\n",
    "count = 0\n",
    "for index, item in user_list:\n",
    "    # split item in train and test data\n",
    "    train, test = train_test_split(item, test_size = 0.3, random_state = 1601)\n",
    "    user = train\n",
    "    # library implemenatation\n",
    "#     xopt, fopt = pyswarm.pso(fitness_func, lower_bound, upper_bound)\n",
    "    # Anubhav's implementation\n",
    "    xopt, fopt = pso(fitness_func, lower_bound, upper_bound)\n",
    "    print(xopt)\n",
    "    print(fopt)\n",
    "    # predict the values on the test dataset\n",
    "    predicted_list = predict_pso(xopt, test)\n",
    "#     print(\"test shape \", test.shape[0])\n",
    "    # calculate the mean average error\n",
    "#     print(\"checking --> \",test.overall.shape[1])\n",
    "#     print(\"debug = \",len(predicted_list))\n",
    "#     print(\"debug = \", len(test.overall))\n",
    "    error = metrics.mean_absolute_error(test.overall.tolist(), predicted_list)\n",
    "    accuracy = metrics.r2_score(test.overall.tolist(), predicted_list)\n",
    "    print(\"mae = \",error)\n",
    "    mae += error\n",
    "    count += 1\n",
    "    if(count==20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-209dfa872c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mae = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mae' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"mae = \",mae/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "## (using library provided by sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "total =  1000\n",
      "below average =  715\n",
      "total =  1000\n",
      "below average =  768\n",
      "total =  1000\n",
      "below average =  725\n",
      "total =  1000\n",
      "below average =  750\n",
      "total =  1000\n",
      "below average =  723\n",
      "total =  1078\n",
      "below average =  779\n"
     ]
    }
   ],
   "source": [
    "# applying linear regression on list_datasets\n",
    "# plotting style\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "print(len(list_datasets))\n",
    "for i in range(len(list_datasets)):\n",
    "    df = list_datasets[i]\n",
    "    df_group = df.groupby(['user_id'])\n",
    "    count = 0\n",
    "    avg_error = 0\n",
    "#     print(\"mae = \",avg_error/len(list_datasets[i].user_id.unique()))\n",
    "    flag = 0\n",
    "    # for plotting graph\n",
    "#     fig = plt.figure()\n",
    "#     ax = plt.axes()\n",
    "    for key,item in df_group:\n",
    "        # splitting the item in train and test data\n",
    "        train, test = train_test_split(item, test_size = 0.2, random_state=1212)\n",
    "        train_X = train[['criterion1', 'criterion2', 'criterion3', 'criterion4']]\n",
    "        train_y = train.overall\n",
    "        test_X = test[['criterion1', 'criterion2', 'criterion3', 'criterion4']]\n",
    "        test_y = test.overall\n",
    "        # create linear regression object\n",
    "        clf = linear_model.LinearRegression(normalize=True)\n",
    "        # train the model using the training sets\n",
    "        clf.fit(train_X, train_y)\n",
    "        prediction = clf.predict(test_X)\n",
    "        st=0\n",
    "        # checking the accuracy\n",
    "        score = clf.score(test_X, test_y)\n",
    "        \n",
    "#         print(score)\n",
    "        \n",
    "        if(score<0.80 and flag==0):\n",
    "            count+=1\n",
    "#             flag=1\n",
    "            # plotting the learning curve\n",
    "#             r,s,t = learning_curve(linear_model.LinearRegression(), item[['criterion1', 'criterion2', 'criterion3', 'criterion4']], item.overall)\n",
    "#             plt.plot(r,s)\n",
    "#             plt = plot_learning_curve(linear_model.LinearRegression(),\"My curve\" ,item[['criterion1', 'criterion2', 'criterion3', 'criterion4']], item.overall)    \n",
    "#             plt.plot(r,t)\n",
    "#             plt.show()\n",
    "    print(\"total = \",len(df_group))\n",
    "    print(\"below average = \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to predict weights of a particular user using Linear Regression\n",
    "# it also returns the trained classifier\n",
    "def predict_weights(df, user):\n",
    "    user_df = df[df['user_id']==user]\n",
    "    user_df_X = user_df[['criterion1', 'criterion2', 'criterion3', 'criterion4']]\n",
    "    user_df_y = user_df.overall\n",
    "    # create linear regression object\n",
    "    clf = linear_model.LinearRegression(normalize=True)\n",
    "    # train the model using the training sets\n",
    "    clf.fit(user_df_X, user_df_y)\n",
    "#     prediction = clf.predict(test_X)\n",
    "    # checking the accuracy\n",
    "    return clf, clf.coef_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions for finding similarity between two users based on their preferences to individual criteria\n",
    "# based on euclidean distance\n",
    "# where weights is a list of individual weights of user1\n",
    "def euclidean_similarity_weighted(df, user1, user2, weights):\n",
    "    s = pd.merge(df[df['user_id']==user1], df[df['user_id']==user2], how=\"inner\", on=\"movie_id\")\n",
    "    sum1 = 0\n",
    "    similarity=0\n",
    "    w1 = weights[0]\n",
    "    w2 = weights[1]\n",
    "    w3 = weights[2]\n",
    "    w4 = weights[3]\n",
    "    for index, row in s.iterrows():\n",
    "        sum1 += math.sqrt(w1*((row.criterion1_x - row.criterion1_y)**2) + \\\n",
    "                w2*((row.criterion2_x - row.criterion2_y)**2) + \\\n",
    "                w3*((row.criterion3_x - row.criterion3_y)**2) + \\\n",
    "                w4*((row.criterion4_x - row.criterion4_y)**2)) + \\\n",
    "                (row.overall_x - row.overall_y)**2\n",
    "        distance = sum1/s.shape[0]\n",
    "        similarity = 1/(1+distance)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to make predictions using some evolutionary method\n",
    "def predict(df, user, item, similarity = euclidean_similarity_weighted, neighbours = -1): # df is the train dataset\n",
    "    neighbours_data_list = list()\n",
    "    for v in df.user_id.unique():\n",
    "        if(v==user): # not including the user itself\n",
    "            continue\n",
    "        temp = df[df['user_id']==v]\n",
    "        temp = temp[temp['movie_id']==item]\n",
    "        if(temp.empty): # user 'v' has not rated the item\n",
    "            continue\n",
    "        else:\n",
    "            rate = temp.iloc[0].overall\n",
    "        \n",
    "        # get the weights preidcted using linear regression\n",
    "        clf, coef = predict_weights()\n",
    "        sim = similarity(df, user, v, coef) # find similarity between the two users\n",
    "        neighbours_data_list.append((sim, rate))\n",
    "        \n",
    "    # sort the neighbours_data_list in descending order based on rate\n",
    "    neighbours_data_list.sort(reverse=True)\n",
    "    \n",
    "    # crop the list to the number of neighbours given in the argument\n",
    "    if(neighbours!=-1):\n",
    "        length = len(neighbours_data_list)\n",
    "        neighbours_data_list = neighbours_data_list[:min(neighbours, length)]\n",
    "    \n",
    "    # predict the rating using collaborative filtering formula\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    if(len(neighbours_data_list)==0):\n",
    "        print(\"We cannot predict\")\n",
    "    for tup in neighbours_data_list:\n",
    "        numerator += tup[0]*tup[1]\n",
    "        denominator += tup[0]\n",
    "    if(denominator==0):\n",
    "        return 0\n",
    "    predicted_rating = numerator/denominator\n",
    "    return predicted_rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing the linear regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
