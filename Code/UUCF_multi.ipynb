{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cross_validation import train_test_split # to split the dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the ratings data in pandas dataframe\n",
    "ratings = pd.read_csv('../Yahoo_movies_multi-criteria/data_movies.txt', sep='\\t',names=['user_id', 'criterion1', 'criterion2', 'criterion3', 'criterion4', 'overall', 'movie_id', 'num'])\n",
    "# train, test = sample_split(ratings)\n",
    "\n",
    "# print(train)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6078"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.user_id.drop_duplicates().count() #number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the datasets into smaller datasets\n",
    "def split_dataset(ratings):\n",
    "        df_group = ratings.groupby(['user_id'])\n",
    "        list_datasets = list() # list containing all the small datasets\n",
    "        temp = pd.DataFrame()\n",
    "        count=0\n",
    "        flag=0\n",
    "        for key, item in df_group:\n",
    "            count+=1\n",
    "            temp = temp.append(item)\n",
    "            if count==1000:\n",
    "                if(len(list_datasets)==5 and flag!=1):\n",
    "                    flag=1\n",
    "                    count = count - 78\n",
    "                    continue\n",
    "                list_datasets.append(temp)\n",
    "                temp = pd.DataFrame()\n",
    "                count=0\n",
    "        return list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting a sample into groups userwise\n",
    "def sample_split(dataFrame):\n",
    "    df_group = dataFrame.groupby('user_id')\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "    \n",
    "    for key, item in df_group:\n",
    "        train, test = split_train_test(item, train, test)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting sample in 70% training data and 30% testing data\n",
    "def split_train_test(dataFrame, train, test):\n",
    "    temp_train, temp_test = train_test_split(dataFrame, test_size = 0.3, random_state=1212)# in this our main data is split into train and test\n",
    "    # the attribute test_size=0.3 splits the data into 70% and 30% ratio. train=70% and test=30%\n",
    "    train = train.append(temp_train)\n",
    "    test = test.append(temp_test)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list conatining all the datasets\n",
    "list_datasets = split_dataset(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and test data\n",
    "train, test = sample_split(list_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to find similarity between two users based on Manhattan distance\n",
    "def manhattan_similarity(df, user1, user2):\n",
    "    s = pd.merge(df[df['user_id']==user1], df[df['user_id']==user2], how=\"inner\", on=\"movie_id\")\n",
    "#     print(s)\n",
    "    sum1 = 0\n",
    "    similarity=0\n",
    "    for index, row in s.iterrows():\n",
    "        sum1 += abs(row.criterion1_x - row.criterion1_y) + \\\n",
    "                abs(row.criterion2_x - row.criterion2_y) + \\\n",
    "                abs(row.criterion3_x - row.criterion3_y) + \\\n",
    "                abs(row.criterion4_x - row.criterion4_y) + \\\n",
    "                abs(row.overall_x - row.overall_y)\n",
    "        distance = sum1/s.shape[0]\n",
    "        similarity = 1/(1+distance)\n",
    "    return similarity\n",
    "        \n",
    "manhattan_similarity(train, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06319010128182817"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to find simialrity between two users based on Euclidean distance\n",
    "def euclidean_similarity(df, user1, user2):\n",
    "    s = pd.merge(df[df['user_id']==user1], df[df['user_id']==user2], how=\"inner\", on=\"movie_id\")\n",
    "#     print(s)\n",
    "    sum1 = 0\n",
    "    similarity=0\n",
    "    for index, row in s.iterrows():\n",
    "        sum1 += math.sqrt((row.criterion1_x - row.criterion1_y)**2 + \\\n",
    "                (row.criterion2_x - row.criterion2_y)**2 + \\\n",
    "                (row.criterion3_x - row.criterion3_y)**2 + \\\n",
    "                (row.criterion4_x - row.criterion4_y)**2) + \\\n",
    "                (row.overall_x - row.overall_y)**2\n",
    "        distance = sum1/s.shape[0]\n",
    "        similarity = 1/(1+distance)\n",
    "    return similarity\n",
    "euclidean_similarity(train, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to find simialrity between two users based on Chebyshev distance\n",
    "def chebyshev_similarity(df, user1, user2):\n",
    "    s = pd.merge(df[df['user_id']==user1], df[df['user_id']==user2], how=\"inner\", on=\"movie_id\")\n",
    "#     print(s)\n",
    "    sum1 = 0\n",
    "    similarity=0\n",
    "    for index, row in s.iterrows():\n",
    "        sum1 += max(abs(row.criterion1_x - row.criterion1_y), \\\n",
    "                abs(row.criterion2_x - row.criterion2_y), \\\n",
    "                abs(row.criterion3_x - row.criterion3_y), \\\n",
    "                abs(row.criterion4_x - row.criterion4_y), \\\n",
    "                abs(row.overall_x - row.overall_y))\n",
    "        distance = sum1/s.shape[0]\n",
    "        similarity = 1/(1+distance)\n",
    "    return similarity\n",
    "        \n",
    "chebyshev_similarity(train, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the rating given by user to item\n",
    "# neighbours = -1 implies that consider similarity with all possible users\n",
    "def predict(df, user, item, similarity, neighbours = -1): # df is the train dataset\n",
    "    neighbours_data_list = list()\n",
    "    for v in df.user_id.unique():\n",
    "        if(v==user): # not including the user itself\n",
    "            continue\n",
    "        temp = df[df['user_id']==v]\n",
    "        temp = temp[temp['movie_id']==item]\n",
    "        if(temp.empty): # user 'v' has not rated the item\n",
    "            continue\n",
    "        else:\n",
    "            rate = temp.iloc[0].overall\n",
    "        sim = similarity(df, user, v) # find appropriate similarity measure between the two users\n",
    "        neighbours_data_list.append((sim, rate))\n",
    "        \n",
    "    # sort the neighbours_data_list in descending order based on rate\n",
    "    neighbours_data_list.sort(reverse=True)\n",
    "    \n",
    "    # crop the list to the number of neighbours given in the argument\n",
    "    if(neighbours!=-1):\n",
    "        length = len(neighbours_data_list)\n",
    "        neighbours_data_list = neighbours_data_list[:min(neighbours, length)]\n",
    "    \n",
    "    # predict the rating using collaborative filtering formula\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for tup in neighbours_data_list:\n",
    "        numerator += tup[0]*tup[1]\n",
    "        denominator += tup[0]\n",
    "    predicted_rating = numerator/denominator\n",
    "    return predicted_rating\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id  criterion1  criterion2  criterion3  criterion4  overall  \\\n",
      "3         1           6           6           6           5        5   \n",
      "4         1          10          11          10           9       10   \n",
      "24        1           3          10           6           6        3   \n",
      "45        1           1           3           3           1        1   \n",
      "7         1          12          12          10          12       11   \n",
      "43        1           8          11          10          11       11   \n",
      "39        1           4           8           6          12        3   \n",
      "29        1           7           8           6           9        6   \n",
      "9         1          12           8           8          10       10   \n",
      "28        1          12          11          13          11       12   \n",
      "\n",
      "    movie_id  num  \n",
      "3         86    4  \n",
      "4        132    5  \n",
      "24       581   25  \n",
      "45       919   46  \n",
      "7        191    8  \n",
      "43       879   44  \n",
      "39       842   40  \n",
      "29       685   30  \n",
      "9        232   10  \n",
      "28       638   29  \n"
     ]
    }
   ],
   "source": [
    "print(test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.37336757999115\n",
      "9.959976730657361\n",
      "9.569398389388915\n"
     ]
    }
   ],
   "source": [
    "# print(train)\n",
    "print(predict(train, 1, 879, euclidean_similarity))\n",
    "print(predict(train, 1, 879, manhattan_similarity))\n",
    "print(predict(train, 1, 879, chebyshev_similarity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
